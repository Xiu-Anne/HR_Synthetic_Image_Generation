{
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "name": "",
  "signature": "sha256:261bc3b828f362bd2c851d9ef78a7b2cf71992271e90cd4977f3003d6e1c73f8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import argparse\n",
      "import os\n",
      "import numpy as np\n",
      "import math\n",
      "\n",
      "import torchvision.transforms as transforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "from torch.utils.data import DataLoader\n",
      "from torchvision import datasets\n",
      "from torch.autograd import Variable\n",
      "\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch"
     ],
     "language": "python",
     "metadata": {
      "id": "GdSAvDGyRC_C"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from google.colab import drive\n",
      "drive.mount('/content/drive')"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "5U7xRe-31Gvr",
      "outputId": "6b179530-a4a0-40cf-8172-e84253b1fa5a"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir('/content/drive/MyDrive/Machine_Learning/GAN') #/\n",
      "!pwd\n",
      "%ls"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "XEi2iMTk1im0",
      "outputId": "2ab86a54-8b9b-4acc-ba00-6c0981f21033"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/content/drive/MyDrive/Machine_Learning/GAN\n",
        " avg_losses                 GAN_Pytorch.ipynb               maps_256.npz\n",
        "'Copy of GAN_Tf_v2.ipynb'   GAN_Tf_v2.ipynb                 \u001b[0m\u001b[01;34mmaps_result\u001b[0m/\n",
        " cycle_gan.ipynb            \u001b[01;34minput\u001b[0m/                          \u001b[01;34mMnist\u001b[0m/\n",
        " \u001b[01;34mdiv2k\u001b[0m/                     ISR_Prediction_Tutorial.ipynb   \u001b[01;34mresult\u001b[0m/\n",
        " GAN_ISR.ipynb              ISR_Traininig_Tutorial.ipynb    \u001b[01;34mVar_AEC\u001b[0m/\n",
        " GAN_Keras.ipynb            \u001b[01;34mmaps\u001b[0m/\n"
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skimage import io\n",
      "import cv2\n",
      "from skimage.color import rgb2gray, gray2rgb\n",
      "import pathlib\n",
      "\n",
      "dir_data = pathlib.Path('div2k')\n",
      "path = dir_data/'DIV2K_train_HR'\n",
      "\n",
      "i_0=0 #1rst image to read\n",
      "i_1=255#255 #last image to read\n",
      "\n",
      "#Shape of an image\n",
      "img_2 = io.imread(str(path/ '%s%05d.tif') %(\"D10_H30_XZ_\",i_0))\n",
      "a1, b1, c1= img_2.shape\n",
      "print('Image shape: %d, %d, %d' %(a1,b1,c1))\n",
      "\n",
      "#Initialisation\n",
      "#train_images=np.zeros((i_1-i_0+1,a1,b1,1))\n",
      "train_images=np.zeros((i_1-i_0+1,128,128,3))\n",
      "#Read images\n",
      "for i in range (i_0, i_1+1):    \n",
      "    img= io.imread(str(path / '%s%05d.tif') %(\"D10_H30_XZ_\", i)) \n",
      "    train_images[i-i_0,:,:,:]= cv2.resize(img,(128,128),interpolation = cv2.INTER_LINEAR)\n",
      "print(path,\"read\")\n",
      "print(train_images.shape)\n",
      "\n",
      "train_images = (train_images / 255).astype('float32')\n",
      "#train_images = (train_images).astype('float32')\n",
      "\n",
      "train_images = np.transpose(train_images, (0, 3, 1, 2))\n",
      "print(train_images.shape)"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "R-5waT42GbLK",
      "outputId": "0c942d46-754a-480c-c443-698c5ae2167b"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Image shape: 600, 600, 3\n",
        "div2k/DIV2K_train_HR read\n",
        "(256, 128, 128, 3)\n",
        "(256, 3, 128, 128)\n"
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir('Mnist')\n",
      "os.makedirs(\"images\", exist_ok=True)"
     ],
     "language": "python",
     "metadata": {
      "id": "6lNA9grgM87c"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.argv=['']\n",
      "del sys\n",
      "\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--n_epochs\", type=int, default=1900, help=\"number of epochs of training\")\n",
      "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
      "parser.add_argument(\"--lr\", type=float, default=0.00001, help=\"adam: learning rate\")\n",
      "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
      "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
      "parser.add_argument(\"--n_cpu\", type=int, default=1, help=\"number of cpu threads to use during batch generation\")\n",
      "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
      "parser.add_argument(\"--img_size\", type=int, default=128, help=\"size of each image dimension\")\n",
      "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
      "parser.add_argument(\"--sample_interval\", type=int, default=30, help=\"interval betwen image samples\")\n",
      "opt = parser.parse_args()\n",
      "print(opt)\n",
      "\n",
      "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
      "\n",
      "cuda = True if torch.cuda.is_available() else False"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "dfGQMCUH07M8",
      "outputId": "09badce4-1e6e-4434-b82d-1fc02861e5e5"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=3, img_size=128, latent_dim=100, lr=1e-05, n_cpu=1, n_epochs=1900, sample_interval=30)\n"
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#https://towardsdatascience.com/building-your-own-self-attention-gans-e8c9b9fe8e51\n",
      "class Self_Attn(nn.Module):\n",
      "    \"\"\" Self attention Layer\"\"\"\n",
      "    def __init__(self, in_dim):\n",
      "        super().__init__()\n",
      "        \n",
      "        # Construct the conv layers\n",
      "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)#//2\n",
      "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\n",
      "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
      "        \n",
      "        # Initialize gamma as 0\n",
      "        self.gamma = nn.Parameter(torch.zeros(1))\n",
      "        self.softmax  = nn.Softmax(dim=-1)\n",
      "        \n",
      "    def forward(self,x):\n",
      "        \"\"\"\n",
      "            inputs :\n",
      "                x : input feature maps( B * C * W * H)\n",
      "            returns :\n",
      "                out : self attention value + input feature \n",
      "                attention: B * N * N (N is Width*Height)\n",
      "        \"\"\"\n",
      "        m_batchsize,C,width ,height = x.size()\n",
      "        \n",
      "        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C\n",
      "        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n",
      "        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product\n",
      "        \n",
      "        attention = self.softmax(energy) # B * N * N\n",
      "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n",
      "        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product\n",
      "        out = out.view(m_batchsize,C,width,height) # B * C * W * H\n",
      "        \n",
      "        # Add attention weights onto input\n",
      "        out = self.gamma*out + x\n",
      "        return out#, attention"
     ],
     "language": "python",
     "metadata": {
      "id": "bos7ikTn8FYr"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Generator(nn.Module):\n",
      "    def __init__(self, out_channels=1):\n",
      "        super(Generator, self).__init__()\n",
      "        self.main = nn.Sequential(\n",
      "            # input is Z, going into a convolution\n",
      "            nn.ConvTranspose2d(opt.latent_dim, 64, 4, 1, 0),#, bias=False\n",
      "            #nn.BatchNorm2d(64*16),\n",
      "            nn.ReLU(True),\n",
      "            # state size. 64*16 x 4 x 4\n",
      "            nn.ConvTranspose2d(64, 64, 4, 2, 1),\n",
      "            #nn.BatchNorm2d(64*8),\n",
      "            nn.ReLU(True),\n",
      "            # state size. 64*8 x 8 x 8\n",
      "            nn.ConvTranspose2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64*4),\n",
      "            #nn.Dropout(0.2),\n",
      "            nn.ReLU(True),\n",
      "            #self attention\n",
      "            #Self_Attn(64),\n",
      "            # state size. 64*4 x 16 x 16\n",
      "            nn.ConvTranspose2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64*2),\n",
      "            #nn.Dropout(0.2),\n",
      "            nn.ReLU(True),\n",
      "            # state size. 64*2 x 32 x 32\n",
      "            nn.ConvTranspose2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64),\n",
      "            #nn.Dropout(0.2),\n",
      "            nn.ReLU(True),\n",
      "            # state size. 64 x 64 x 64            \n",
      "            nn.ConvTranspose2d(64, opt.channels, 4, 2, 1, ),\n",
      "            nn.Tanh()\n",
      "            # state size. (nc) x 128 x 128\n",
      "        )\n",
      "\n",
      "    def forward(self, input):\n",
      "        return self.main(input)"
     ],
     "language": "python",
     "metadata": {
      "id": "4ZgKol1B8mQ-"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Discriminator(nn.Module):\n",
      "    def __init__(self, out_channels=1):\n",
      "        super(Discriminator, self).__init__()\n",
      "        self.main = nn.Sequential(\n",
      "            # input is (out_channels) x 128 x 128\n",
      "            nn.Conv2d(opt.channels, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64),\n",
      "            nn.LeakyReLU(0.2, inplace=True),\n",
      "            # state size. (64) x 64 x 64\n",
      "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64),#*2\n",
      "            #nn.Dropout(0.2),\n",
      "            nn.LeakyReLU(0.2, inplace=True),\n",
      "            # state size. 64*4 x 32 x 32\n",
      "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64),#*4\n",
      "            #nn.Dropout(0.2),\n",
      "            nn.LeakyReLU(0.2, inplace=True),\n",
      "            # state size. 64*8 x 16 x 15\n",
      "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64),#*8\n",
      "            #nn.Dropout(0.2),\n",
      "            nn.LeakyReLU(0.2, inplace=True),            \n",
      "            # state size. 64*16 x 8 x 8\n",
      "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
      "            #nn.BatchNorm2d(64),#*16\n",
      "            #n.Dropout(0.2),\n",
      "            nn.LeakyReLU(0.2, inplace=True),\n",
      "            #self attention\n",
      "            #Self_Attn(64),\n",
      "            # state size. 64*16 x 4 x 4\n",
      "            nn.Conv2d(64, 1, 4, 1, 0, ),\n",
      "            #nn.Dropout(0.5),\n",
      "            nn.Flatten(),\n",
      "            nn.Sigmoid()\n",
      "        )\n",
      "\n",
      "    def forward(self, input):\n",
      "        crit_pred = self.main(input)\n",
      "        return crit_pred.view(len(crit_pred), -1)"
     ],
     "language": "python",
     "metadata": {
      "id": "-qx2OCZz-oRV"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initialize generator and discriminator\n",
      "from torchsummary import summary\n",
      "generator = Generator()\n",
      "discriminator = Discriminator()\n",
      "\n",
      "# Loss function\n",
      "adversarial_loss = torch.nn.BCELoss() #BCEWithLogitsLoss()#\n",
      "\n",
      "if cuda:\n",
      "    generator.cuda()\n",
      "    discriminator.cuda()\n",
      "    adversarial_loss.cuda()\n",
      "\n",
      "summary(generator, (opt.latent_dim,1,1))\n",
      "summary(discriminator, (3,128,128))\n",
      "\n",
      "# Configure data loader\n",
      "#os.makedirs(\"Mnist\", exist_ok=True)\n",
      "dataloader = torch.utils.data.DataLoader(train_images, batch_size=opt.batch_size, num_workers=opt.n_cpu, shuffle=True)\n",
      "\n",
      "# Optimizers\n",
      "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
      "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
      "\n",
      "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/"
      },
      "id": "5KRLcWvUB3Un",
      "outputId": "9aa52723-c69c-431c-b8a9-94a05b4cd0e3"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "   ConvTranspose2d-1             [-1, 64, 4, 4]         102,464\n",
        "              ReLU-2             [-1, 64, 4, 4]               0\n",
        "   ConvTranspose2d-3             [-1, 64, 8, 8]          65,600\n",
        "              ReLU-4             [-1, 64, 8, 8]               0\n",
        "   ConvTranspose2d-5           [-1, 64, 16, 16]          65,600\n",
        "              ReLU-6           [-1, 64, 16, 16]               0\n",
        "   ConvTranspose2d-7           [-1, 64, 32, 32]          65,600\n",
        "              ReLU-8           [-1, 64, 32, 32]               0\n",
        "   ConvTranspose2d-9           [-1, 64, 64, 64]          65,600\n",
        "             ReLU-10           [-1, 64, 64, 64]               0\n",
        "  ConvTranspose2d-11          [-1, 3, 128, 128]           3,075\n",
        "             Tanh-12          [-1, 3, 128, 128]               0\n",
        "================================================================\n",
        "Total params: 367,939\n",
        "Trainable params: 367,939\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 6.08\n",
        "Params size (MB): 1.40\n",
        "Estimated Total Size (MB): 7.48\n",
        "----------------------------------------------------------------\n",
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "            Conv2d-1           [-1, 64, 64, 64]           3,136\n",
        "         LeakyReLU-2           [-1, 64, 64, 64]               0\n",
        "            Conv2d-3           [-1, 64, 32, 32]          65,600\n",
        "         LeakyReLU-4           [-1, 64, 32, 32]               0\n",
        "            Conv2d-5           [-1, 64, 16, 16]          65,600\n",
        "         LeakyReLU-6           [-1, 64, 16, 16]               0\n",
        "            Conv2d-7             [-1, 64, 8, 8]          65,600\n",
        "         LeakyReLU-8             [-1, 64, 8, 8]               0\n",
        "            Conv2d-9             [-1, 64, 4, 4]          65,600\n",
        "        LeakyReLU-10             [-1, 64, 4, 4]               0\n",
        "           Conv2d-11              [-1, 1, 1, 1]           1,025\n",
        "          Flatten-12                    [-1, 1]               0\n",
        "          Sigmoid-13                    [-1, 1]               0\n",
        "================================================================\n",
        "Total params: 266,561\n",
        "Trainable params: 266,561\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 0.19\n",
        "Forward/backward pass size (MB): 5.33\n",
        "Params size (MB): 1.02\n",
        "Estimated Total Size (MB): 6.53\n",
        "----------------------------------------------------------------\n"
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ----------\n",
      "#  Training\n",
      "# ----------\n",
      "\n",
      "for epoch in range(opt.n_epochs):\n",
      "    #for i, (imgs, _) in enumerate(dataloader):\n",
      "    for i, imgs in enumerate(dataloader):\n",
      "\n",
      "        # Adversarial ground truths\n",
      "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
      "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
      "\n",
      "        # Configure input\n",
      "        real_imgs = Variable(imgs.type(Tensor))\n",
      "        real_imgs = torch.reshape(real_imgs, (imgs.size(0), opt.channels, 128, 128))\n",
      "\n",
      "        # ---------------------\n",
      "        #  Train Discriminator\n",
      "        # ---------------------\n",
      "\n",
      "        # Sample noise as generator input\n",
      "        #z = torch.randn(opt.latent_dim)\n",
      "        z = Variable(torch.randn(opt.batch_size,opt.latent_dim, 1, 1, device=DEVICE))#\n",
      "        # Generate a batch of images\n",
      "        gen_imgs = generator(z)\n",
      "\n",
      "        optimizer_D.zero_grad()\n",
      "\n",
      "        # Measure discriminator's ability to classify real from generated samples\n",
      "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
      "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
      "        d_loss = (real_loss + fake_loss) #/ 2\n",
      "\n",
      "        d_loss.backward()\n",
      "        optimizer_D.step()\n",
      "\n",
      "        # -----------------\n",
      "        #  Train Generator\n",
      "        # -----------------\n",
      "\n",
      "        optimizer_G.zero_grad()\n",
      "\n",
      "        z_2 = Variable(torch.randn(opt.batch_size,opt.latent_dim, 1, 1, device=DEVICE))#\n",
      "        # Generate a batch of images\n",
      "        gen_imgs_2 = generator(z_2)\n",
      "\n",
      "        # Loss measures generator's ability to fool the discriminator\n",
      "        g_loss = adversarial_loss(discriminator(gen_imgs_2), valid)\n",
      "\n",
      "        g_loss.backward()\n",
      "        optimizer_G.step()\n",
      "\n",
      "\n",
      "        if epoch % 10 == 0:\n",
      "            print(\n",
      "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
      "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
      "            )\n",
      "\n",
      "        #batches_done = epoch * len(dataloader) + i\n",
      "        if epoch % opt.sample_interval == 0:\n",
      "            save_image(gen_imgs.data[:25], \"images/%d.png\" % epoch, nrow=5, normalize=True)"
     ],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/",
       "height": 1000
      },
      "id": "NI3ByZ4Bgb2h",
      "outputId": "a33b4906-eeb0-4c6c-ab68-fe9f268f7120"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Epoch 0/1900] [Batch 0/4] [D loss: 0.018309] [G loss: 5.060669]\n",
        "[Epoch 0/1900] [Batch 1/4] [D loss: 0.050665] [G loss: 3.720486]\n",
        "[Epoch 0/1900] [Batch 2/4] [D loss: 0.050574] [G loss: 5.648841]\n",
        "[Epoch 0/1900] [Batch 3/4] [D loss: 0.043184] [G loss: 5.056637]\n",
        "[Epoch 10/1900] [Batch 0/4] [D loss: 0.040033] [G loss: 3.579831]\n",
        "[Epoch 10/1900] [Batch 1/4] [D loss: 0.044748] [G loss: 6.021144]\n",
        "[Epoch 10/1900] [Batch 2/4] [D loss: 0.060782] [G loss: 4.236419]\n",
        "[Epoch 10/1900] [Batch 3/4] [D loss: 0.027150] [G loss: 4.437761]\n",
        "[Epoch 20/1900] [Batch 0/4] [D loss: 0.042221] [G loss: 4.463952]\n",
        "[Epoch 20/1900] [Batch 1/4] [D loss: 0.025047] [G loss: 5.181932]\n",
        "[Epoch 20/1900] [Batch 2/4] [D loss: 0.067705] [G loss: 4.461270]\n",
        "[Epoch 20/1900] [Batch 3/4] [D loss: 0.042682] [G loss: 4.372454]\n",
        "[Epoch 30/1900] [Batch 0/4] [D loss: 0.015602] [G loss: 5.022284]\n",
        "[Epoch 30/1900] [Batch 1/4] [D loss: 0.022609] [G loss: 4.923883]\n",
        "[Epoch 30/1900] [Batch 2/4] [D loss: 0.010626] [G loss: 5.382317]\n",
        "[Epoch 30/1900] [Batch 3/4] [D loss: 0.021581] [G loss: 5.119013]\n",
        "[Epoch 40/1900] [Batch 0/4] [D loss: 0.086379] [G loss: 7.716540]\n",
        "[Epoch 40/1900] [Batch 1/4] [D loss: 0.098334] [G loss: 5.700377]\n",
        "[Epoch 40/1900] [Batch 2/4] [D loss: 0.017630] [G loss: 4.638126]\n",
        "[Epoch 40/1900] [Batch 3/4] [D loss: 0.043750] [G loss: 5.702637]\n",
        "[Epoch 50/1900] [Batch 0/4] [D loss: 0.032455] [G loss: 6.310274]\n",
        "[Epoch 50/1900] [Batch 1/4] [D loss: 0.047828] [G loss: 4.853378]\n",
        "[Epoch 50/1900] [Batch 2/4] [D loss: 0.028181] [G loss: 4.008535]\n",
        "[Epoch 50/1900] [Batch 3/4] [D loss: 0.031057] [G loss: 5.901477]\n",
        "[Epoch 60/1900] [Batch 0/4] [D loss: 0.375298] [G loss: 29.703087]\n",
        "[Epoch 60/1900] [Batch 1/4] [D loss: 1.708074] [G loss: 27.892937]\n",
        "[Epoch 60/1900] [Batch 2/4] [D loss: 1.849249] [G loss: 15.062126]\n",
        "[Epoch 60/1900] [Batch 3/4] [D loss: 0.317759] [G loss: 3.807738]\n",
        "[Epoch 70/1900] [Batch 0/4] [D loss: 0.051289] [G loss: 5.071572]\n",
        "[Epoch 70/1900] [Batch 1/4] [D loss: 0.059148] [G loss: 3.749716]\n",
        "[Epoch 70/1900] [Batch 2/4] [D loss: 0.042139] [G loss: 5.942941]\n",
        "[Epoch 70/1900] [Batch 3/4] [D loss: 0.012852] [G loss: 6.665112]\n",
        "[Epoch 80/1900] [Batch 0/4] [D loss: 0.064037] [G loss: 4.306866]\n",
        "[Epoch 80/1900] [Batch 1/4] [D loss: 0.038372] [G loss: 4.556967]\n",
        "[Epoch 80/1900] [Batch 2/4] [D loss: 0.019999] [G loss: 5.829724]\n",
        "[Epoch 80/1900] [Batch 3/4] [D loss: 0.037176] [G loss: 4.880250]\n",
        "[Epoch 90/1900] [Batch 0/4] [D loss: 0.006605] [G loss: 4.807282]\n",
        "[Epoch 90/1900] [Batch 1/4] [D loss: 0.023068] [G loss: 4.847601]\n",
        "[Epoch 90/1900] [Batch 2/4] [D loss: 0.022794] [G loss: 4.930793]\n",
        "[Epoch 90/1900] [Batch 3/4] [D loss: 0.034258] [G loss: 4.850729]\n",
        "[Epoch 100/1900] [Batch 0/4] [D loss: 0.185153] [G loss: 7.538074]\n",
        "[Epoch 100/1900] [Batch 1/4] [D loss: 0.033657] [G loss: 9.337000]\n",
        "[Epoch 100/1900] [Batch 2/4] [D loss: 0.147658] [G loss: 8.478334]\n",
        "[Epoch 100/1900] [Batch 3/4] [D loss: 0.087001] [G loss: 7.070064]\n",
        "[Epoch 110/1900] [Batch 0/4] [D loss: 0.043426] [G loss: 4.278553]\n",
        "[Epoch 110/1900] [Batch 1/4] [D loss: 0.036552] [G loss: 4.339889]\n",
        "[Epoch 110/1900] [Batch 2/4] [D loss: 0.020579] [G loss: 5.329371]\n",
        "[Epoch 110/1900] [Batch 3/4] [D loss: 0.014265] [G loss: 5.742812]\n",
        "[Epoch 120/1900] [Batch 0/4] [D loss: 0.015920] [G loss: 5.077337]\n",
        "[Epoch 120/1900] [Batch 1/4] [D loss: 0.014328] [G loss: 5.279427]\n",
        "[Epoch 120/1900] [Batch 2/4] [D loss: 0.014808] [G loss: 5.471316]\n",
        "[Epoch 120/1900] [Batch 3/4] [D loss: 0.016764] [G loss: 5.209702]\n",
        "[Epoch 130/1900] [Batch 0/4] [D loss: 0.015555] [G loss: 5.362378]\n",
        "[Epoch 130/1900] [Batch 1/4] [D loss: 0.012884] [G loss: 5.746764]\n",
        "[Epoch 130/1900] [Batch 2/4] [D loss: 0.020071] [G loss: 5.304642]\n",
        "[Epoch 130/1900] [Batch 3/4] [D loss: 0.025012] [G loss: 4.494245]\n",
        "[Epoch 140/1900] [Batch 0/4] [D loss: 0.010796] [G loss: 5.644377]\n",
        "[Epoch 140/1900] [Batch 1/4] [D loss: 0.011209] [G loss: 5.820739]\n",
        "[Epoch 140/1900] [Batch 2/4] [D loss: 0.011676] [G loss: 5.784259]\n",
        "[Epoch 140/1900] [Batch 3/4] [D loss: 0.018163] [G loss: 5.254457]\n",
        "[Epoch 150/1900] [Batch 0/4] [D loss: 0.016072] [G loss: 5.455687]\n",
        "[Epoch 150/1900] [Batch 1/4] [D loss: 0.015736] [G loss: 5.582210]\n",
        "[Epoch 150/1900] [Batch 2/4] [D loss: 0.009192] [G loss: 5.904271]\n",
        "[Epoch 150/1900] [Batch 3/4] [D loss: 0.018848] [G loss: 5.326439]\n",
        "[Epoch 160/1900] [Batch 0/4] [D loss: 0.031599] [G loss: 8.549614]\n",
        "[Epoch 160/1900] [Batch 1/4] [D loss: 0.013584] [G loss: 6.276702]\n",
        "[Epoch 160/1900] [Batch 2/4] [D loss: 0.009064] [G loss: 5.017868]\n",
        "[Epoch 160/1900] [Batch 3/4] [D loss: 0.015401] [G loss: 5.320504]\n",
        "[Epoch 170/1900] [Batch 0/4] [D loss: 0.005349] [G loss: 6.128038]\n",
        "[Epoch 170/1900] [Batch 1/4] [D loss: 0.007848] [G loss: 5.875093]\n",
        "[Epoch 170/1900] [Batch 2/4] [D loss: 0.005673] [G loss: 5.942558]\n",
        "[Epoch 170/1900] [Batch 3/4] [D loss: 0.014611] [G loss: 5.426536]\n",
        "[Epoch 180/1900] [Batch 0/4] [D loss: 0.026791] [G loss: 6.468655]\n",
        "[Epoch 180/1900] [Batch 1/4] [D loss: 0.009261] [G loss: 5.324502]\n",
        "[Epoch 180/1900] [Batch 2/4] [D loss: 0.013907] [G loss: 4.748718]\n",
        "[Epoch 180/1900] [Batch 3/4] [D loss: 0.014056] [G loss: 5.207606]\n",
        "[Epoch 190/1900] [Batch 0/4] [D loss: 0.017490] [G loss: 5.259945]\n",
        "[Epoch 190/1900] [Batch 1/4] [D loss: 0.012352] [G loss: 6.054730]\n",
        "[Epoch 190/1900] [Batch 2/4] [D loss: 0.048698] [G loss: 3.915745]\n",
        "[Epoch 190/1900] [Batch 3/4] [D loss: 0.048061] [G loss: 5.809233]\n",
        "[Epoch 200/1900] [Batch 0/4] [D loss: 0.009972] [G loss: 6.194630]\n",
        "[Epoch 200/1900] [Batch 1/4] [D loss: 0.059957] [G loss: 4.903692]\n",
        "[Epoch 200/1900] [Batch 2/4] [D loss: 0.069029] [G loss: 3.362778]\n",
        "[Epoch 200/1900] [Batch 3/4] [D loss: 0.057115] [G loss: 5.259851]\n",
        "[Epoch 210/1900] [Batch 0/4] [D loss: 0.030779] [G loss: 5.189767]\n",
        "[Epoch 210/1900] [Batch 1/4] [D loss: 0.027809] [G loss: 5.235859]\n",
        "[Epoch 210/1900] [Batch 2/4] [D loss: 0.045211] [G loss: 4.076921]\n",
        "[Epoch 210/1900] [Batch 3/4] [D loss: 0.038438] [G loss: 4.771718]\n",
        "[Epoch 220/1900] [Batch 0/4] [D loss: 0.014825] [G loss: 5.986172]\n",
        "[Epoch 220/1900] [Batch 1/4] [D loss: 0.013817] [G loss: 5.869250]\n",
        "[Epoch 220/1900] [Batch 2/4] [D loss: 0.035923] [G loss: 4.261584]\n",
        "[Epoch 220/1900] [Batch 3/4] [D loss: 0.033147] [G loss: 3.842353]\n",
        "[Epoch 230/1900] [Batch 0/4] [D loss: 0.034317] [G loss: 4.899929]\n",
        "[Epoch 230/1900] [Batch 1/4] [D loss: 0.037002] [G loss: 4.901362]\n",
        "[Epoch 230/1900] [Batch 2/4] [D loss: 0.031645] [G loss: 4.576478]\n",
        "[Epoch 230/1900] [Batch 3/4] [D loss: 0.020959] [G loss: 5.121785]\n",
        "[Epoch 240/1900] [Batch 0/4] [D loss: 0.007055] [G loss: 9.058648]\n",
        "[Epoch 240/1900] [Batch 1/4] [D loss: 0.094462] [G loss: 6.200177]\n",
        "[Epoch 240/1900] [Batch 2/4] [D loss: 0.014803] [G loss: 3.646255]\n",
        "[Epoch 240/1900] [Batch 3/4] [D loss: 0.090324] [G loss: 8.447038]\n",
        "[Epoch 250/1900] [Batch 0/4] [D loss: 0.257006] [G loss: 5.877578]\n",
        "[Epoch 250/1900] [Batch 1/4] [D loss: 0.017528] [G loss: 3.692266]\n",
        "[Epoch 250/1900] [Batch 2/4] [D loss: 0.037881] [G loss: 5.478326]\n",
        "[Epoch 250/1900] [Batch 3/4] [D loss: 0.056494] [G loss: 4.309031]\n",
        "[Epoch 260/1900] [Batch 0/4] [D loss: 0.025351] [G loss: 5.803415]\n",
        "[Epoch 260/1900] [Batch 1/4] [D loss: 0.024273] [G loss: 4.956271]\n",
        "[Epoch 260/1900] [Batch 2/4] [D loss: 0.053349] [G loss: 2.820820]\n",
        "[Epoch 260/1900] [Batch 3/4] [D loss: 0.088729] [G loss: 8.822826]\n",
        "[Epoch 270/1900] [Batch 0/4] [D loss: 0.035212] [G loss: 5.093841]\n",
        "[Epoch 270/1900] [Batch 1/4] [D loss: 0.038946] [G loss: 5.160766]\n",
        "[Epoch 270/1900] [Batch 2/4] [D loss: 0.031850] [G loss: 4.498289]\n",
        "[Epoch 270/1900] [Batch 3/4] [D loss: 0.037494] [G loss: 4.625110]\n",
        "[Epoch 280/1900] [Batch 0/4] [D loss: 0.010618] [G loss: 5.652890]\n",
        "[Epoch 280/1900] [Batch 1/4] [D loss: 0.035007] [G loss: 4.435396]\n",
        "[Epoch 280/1900] [Batch 2/4] [D loss: 0.043090] [G loss: 3.947022]\n",
        "[Epoch 280/1900] [Batch 3/4] [D loss: 0.034827] [G loss: 5.407886]\n",
        "[Epoch 290/1900] [Batch 0/4] [D loss: 0.021646] [G loss: 5.181481]\n",
        "[Epoch 290/1900] [Batch 1/4] [D loss: 0.021675] [G loss: 5.995339]\n",
        "[Epoch 290/1900] [Batch 2/4] [D loss: 0.026109] [G loss: 5.408081]\n",
        "[Epoch 290/1900] [Batch 3/4] [D loss: 0.017630] [G loss: 5.290168]\n",
        "[Epoch 300/1900] [Batch 0/4] [D loss: 0.189456] [G loss: 8.733490]\n",
        "[Epoch 300/1900] [Batch 1/4] [D loss: 0.081545] [G loss: 5.390597]\n",
        "[Epoch 300/1900] [Batch 2/4] [D loss: 0.011229] [G loss: 3.999014]\n",
        "[Epoch 300/1900] [Batch 3/4] [D loss: 0.044672] [G loss: 5.394675]\n",
        "[Epoch 310/1900] [Batch 0/4] [D loss: 0.014391] [G loss: 5.866331]\n",
        "[Epoch 310/1900] [Batch 1/4] [D loss: 0.100914] [G loss: 2.104746]\n",
        "[Epoch 310/1900] [Batch 2/4] [D loss: 0.173473] [G loss: 15.385424]\n",
        "[Epoch 310/1900] [Batch 3/4] [D loss: 0.439073] [G loss: 16.321718]\n",
        "[Epoch 320/1900] [Batch 0/4] [D loss: 0.043294] [G loss: 6.196736]\n",
        "[Epoch 320/1900] [Batch 1/4] [D loss: 0.111994] [G loss: 3.066039]\n",
        "[Epoch 320/1900] [Batch 2/4] [D loss: 0.081972] [G loss: 8.054487]\n",
        "[Epoch 320/1900] [Batch 3/4] [D loss: 0.057527] [G loss: 8.323332]\n",
        "[Epoch 330/1900] [Batch 0/4] [D loss: 0.042959] [G loss: 6.006985]\n",
        "[Epoch 330/1900] [Batch 1/4] [D loss: 0.084303] [G loss: 4.790791]\n",
        "[Epoch 330/1900] [Batch 2/4] [D loss: 0.020378] [G loss: 4.528179]\n",
        "[Epoch 330/1900] [Batch 3/4] [D loss: 0.014970] [G loss: 5.459617]\n",
        "[Epoch 340/1900] [Batch 0/4] [D loss: 0.042167] [G loss: 5.082414]\n",
        "[Epoch 340/1900] [Batch 1/4] [D loss: 0.042600] [G loss: 4.888439]\n",
        "[Epoch 340/1900] [Batch 2/4] [D loss: 0.046652] [G loss: 3.942375]\n",
        "[Epoch 340/1900] [Batch 3/4] [D loss: 0.029838] [G loss: 5.028767]\n",
        "[Epoch 350/1900] [Batch 0/4] [D loss: 0.034357] [G loss: 4.424847]\n",
        "[Epoch 350/1900] [Batch 1/4] [D loss: 0.030230] [G loss: 4.352044]\n",
        "[Epoch 350/1900] [Batch 2/4] [D loss: 0.014451] [G loss: 5.516288]\n",
        "[Epoch 350/1900] [Batch 3/4] [D loss: 0.013332] [G loss: 5.988597]\n",
        "[Epoch 360/1900] [Batch 0/4] [D loss: 0.884483] [G loss: 20.670557]\n",
        "[Epoch 360/1900] [Batch 1/4] [D loss: 0.287800] [G loss: 13.840378]\n",
        "[Epoch 360/1900] [Batch 2/4] [D loss: 0.274204] [G loss: 7.696366]\n",
        "[Epoch 360/1900] [Batch 3/4] [D loss: 0.004994] [G loss: 3.932218]\n",
        "[Epoch 370/1900] [Batch 0/4] [D loss: 0.012570] [G loss: 5.903382]\n",
        "[Epoch 370/1900] [Batch 1/4] [D loss: 0.044689] [G loss: 5.071514]\n",
        "[Epoch 370/1900] [Batch 2/4] [D loss: 0.017874] [G loss: 4.663856]\n",
        "[Epoch 370/1900] [Batch 3/4] [D loss: 0.033200] [G loss: 4.350283]\n",
        "[Epoch 380/1900] [Batch 0/4] [D loss: 0.026811] [G loss: 4.600552]\n",
        "[Epoch 380/1900] [Batch 1/4] [D loss: 0.016259] [G loss: 5.124651]\n",
        "[Epoch 380/1900] [Batch 2/4] [D loss: 0.016675] [G loss: 5.237268]\n",
        "[Epoch 380/1900] [Batch 3/4] [D loss: 0.032921] [G loss: 4.344660]\n",
        "[Epoch 390/1900] [Batch 0/4] [D loss: 0.071243] [G loss: 3.466777]\n",
        "[Epoch 390/1900] [Batch 1/4] [D loss: 0.052396] [G loss: 5.521773]\n",
        "[Epoch 390/1900] [Batch 2/4] [D loss: 0.008733] [G loss: 6.532390]\n",
        "[Epoch 390/1900] [Batch 3/4] [D loss: 0.043731] [G loss: 4.775510]\n",
        "[Epoch 400/1900] [Batch 0/4] [D loss: 0.013269] [G loss: 5.444265]\n",
        "[Epoch 400/1900] [Batch 1/4] [D loss: 0.023964] [G loss: 5.036842]\n",
        "[Epoch 400/1900] [Batch 2/4] [D loss: 0.008697] [G loss: 5.337430]\n",
        "[Epoch 400/1900] [Batch 3/4] [D loss: 0.062021] [G loss: 3.599396]\n",
        "[Epoch 410/1900] [Batch 0/4] [D loss: 0.033065] [G loss: 4.290451]\n",
        "[Epoch 410/1900] [Batch 1/4] [D loss: 0.033925] [G loss: 4.373256]\n",
        "[Epoch 410/1900] [Batch 2/4] [D loss: 0.039281] [G loss: 4.499540]\n",
        "[Epoch 410/1900] [Batch 3/4] [D loss: 0.017376] [G loss: 5.530470]\n",
        "[Epoch 420/1900] [Batch 0/4] [D loss: 0.012379] [G loss: 5.379699]\n",
        "[Epoch 420/1900] [Batch 1/4] [D loss: 0.015036] [G loss: 5.215057]\n",
        "[Epoch 420/1900] [Batch 2/4] [D loss: 0.044905] [G loss: 3.490054]\n",
        "[Epoch 420/1900] [Batch 3/4] [D loss: 0.052244] [G loss: 6.355721]\n",
        "[Epoch 430/1900] [Batch 0/4] [D loss: 0.032823] [G loss: 5.992973]\n",
        "[Epoch 430/1900] [Batch 1/4] [D loss: 0.032197] [G loss: 5.263765]\n",
        "[Epoch 430/1900] [Batch 2/4] [D loss: 0.052493] [G loss: 3.665132]\n",
        "[Epoch 430/1900] [Batch 3/4] [D loss: 0.049984] [G loss: 5.687710]\n",
        "[Epoch 440/1900] [Batch 0/4] [D loss: 0.027758] [G loss: 3.458942]\n",
        "[Epoch 440/1900] [Batch 1/4] [D loss: 0.041414] [G loss: 5.331407]\n",
        "[Epoch 440/1900] [Batch 2/4] [D loss: 0.017193] [G loss: 6.051408]\n",
        "[Epoch 440/1900] [Batch 3/4] [D loss: 0.014957] [G loss: 5.861444]\n",
        "[Epoch 450/1900] [Batch 0/4] [D loss: 0.392866] [G loss: 17.885010]\n",
        "[Epoch 450/1900] [Batch 1/4] [D loss: 0.459705] [G loss: 10.090593]\n",
        "[Epoch 450/1900] [Batch 2/4] [D loss: 0.189531] [G loss: 2.171815]\n",
        "[Epoch 450/1900] [Batch 3/4] [D loss: 0.244442] [G loss: 12.867730]\n",
        "[Epoch 460/1900] [Batch 0/4] [D loss: 0.178060] [G loss: 5.449007]\n",
        "[Epoch 460/1900] [Batch 1/4] [D loss: 0.051733] [G loss: 2.331147]\n",
        "[Epoch 460/1900] [Batch 2/4] [D loss: 0.133900] [G loss: 11.619370]\n",
        "[Epoch 460/1900] [Batch 3/4] [D loss: 0.202089] [G loss: 12.650375]\n",
        "[Epoch 470/1900] [Batch 0/4] [D loss: 0.024109] [G loss: 4.554045]\n",
        "[Epoch 470/1900] [Batch 1/4] [D loss: 0.017159] [G loss: 5.151321]\n",
        "[Epoch 470/1900] [Batch 2/4] [D loss: 0.017117] [G loss: 5.288364]\n",
        "[Epoch 470/1900] [Batch 3/4] [D loss: 0.005459] [G loss: 5.965897]\n",
        "[Epoch 480/1900] [Batch 0/4] [D loss: 0.018517] [G loss: 4.870426]\n",
        "[Epoch 480/1900] [Batch 1/4] [D loss: 0.021057] [G loss: 4.590338]\n",
        "[Epoch 480/1900] [Batch 2/4] [D loss: 0.015853] [G loss: 5.672456]\n",
        "[Epoch 480/1900] [Batch 3/4] [D loss: 0.019220] [G loss: 5.405304]\n",
        "[Epoch 490/1900] [Batch 0/4] [D loss: 0.121040] [G loss: 1.687221]\n",
        "[Epoch 490/1900] [Batch 1/4] [D loss: 0.627133] [G loss: 51.902672]\n",
        "[Epoch 490/1900] [Batch 2/4] [D loss: 3.753070] [G loss: 56.447205]\n",
        "[Epoch 490/1900] [Batch 3/4] [D loss: 2.552740] [G loss: 50.890202]\n",
        "[Epoch 500/1900] [Batch 0/4] [D loss: 0.006710] [G loss: 6.853624]\n",
        "[Epoch 500/1900] [Batch 1/4] [D loss: 0.072467] [G loss: 4.421776]\n",
        "[Epoch 500/1900] [Batch 2/4] [D loss: 0.045842] [G loss: 4.910369]\n",
        "[Epoch 500/1900] [Batch 3/4] [D loss: 0.054887] [G loss: 4.462318]\n",
        "[Epoch 510/1900] [Batch 0/4] [D loss: 0.011831] [G loss: 6.222151]\n",
        "[Epoch 510/1900] [Batch 1/4] [D loss: 0.019601] [G loss: 5.786520]\n",
        "[Epoch 510/1900] [Batch 2/4] [D loss: 0.017125] [G loss: 5.642566]\n",
        "[Epoch 510/1900] [Batch 3/4] [D loss: 0.030103] [G loss: 5.374425]\n",
        "[Epoch 520/1900] [Batch 0/4] [D loss: 0.024312] [G loss: 4.934473]\n",
        "[Epoch 520/1900] [Batch 1/4] [D loss: 0.023947] [G loss: 7.045415]\n",
        "[Epoch 520/1900] [Batch 2/4] [D loss: 0.009805] [G loss: 7.220022]\n",
        "[Epoch 520/1900] [Batch 3/4] [D loss: 0.017453] [G loss: 5.782479]\n",
        "[Epoch 530/1900] [Batch 0/4] [D loss: 0.012074] [G loss: 5.845150]\n",
        "[Epoch 530/1900] [Batch 1/4] [D loss: 0.010524] [G loss: 6.096904]\n",
        "[Epoch 530/1900] [Batch 2/4] [D loss: 0.013808] [G loss: 5.854177]\n",
        "[Epoch 530/1900] [Batch 3/4] [D loss: 0.010538] [G loss: 6.284531]\n",
        "[Epoch 540/1900] [Batch 0/4] [D loss: 0.010620] [G loss: 6.040567]\n",
        "[Epoch 540/1900] [Batch 1/4] [D loss: 0.011155] [G loss: 6.609054]\n",
        "[Epoch 540/1900] [Batch 2/4] [D loss: 0.009308] [G loss: 6.834941]\n",
        "[Epoch 540/1900] [Batch 3/4] [D loss: 0.057439] [G loss: 2.793960]\n",
        "[Epoch 550/1900] [Batch 0/4] [D loss: 0.019077] [G loss: 6.708026]\n",
        "[Epoch 550/1900] [Batch 1/4] [D loss: 0.056142] [G loss: 4.680187]\n",
        "[Epoch 550/1900] [Batch 2/4] [D loss: 0.038114] [G loss: 6.118393]\n",
        "[Epoch 550/1900] [Batch 3/4] [D loss: 0.014615] [G loss: 6.744814]\n",
        "[Epoch 560/1900] [Batch 0/4] [D loss: 0.026952] [G loss: 5.996567]\n",
        "[Epoch 560/1900] [Batch 1/4] [D loss: 0.022352] [G loss: 6.113376]\n",
        "[Epoch 560/1900] [Batch 2/4] [D loss: 0.021224] [G loss: 5.940386]\n",
        "[Epoch 560/1900] [Batch 3/4] [D loss: 0.019751] [G loss: 5.789285]\n",
        "[Epoch 570/1900] [Batch 0/4] [D loss: 0.006469] [G loss: 6.259630]\n",
        "[Epoch 570/1900] [Batch 1/4] [D loss: 0.005759] [G loss: 6.365008]\n",
        "[Epoch 570/1900] [Batch 2/4] [D loss: 0.009440] [G loss: 6.095163]\n",
        "[Epoch 570/1900] [Batch 3/4] [D loss: 0.009909] [G loss: 5.864349]\n",
        "[Epoch 580/1900] [Batch 0/4] [D loss: 0.005153] [G loss: 6.457588]\n",
        "[Epoch 580/1900] [Batch 1/4] [D loss: 0.006684] [G loss: 6.303240]\n",
        "[Epoch 580/1900] [Batch 2/4] [D loss: 0.005781] [G loss: 6.147874]\n",
        "[Epoch 580/1900] [Batch 3/4] [D loss: 0.007055] [G loss: 6.018793]\n",
        "[Epoch 590/1900] [Batch 0/4] [D loss: 0.005467] [G loss: 6.419594]\n",
        "[Epoch 590/1900] [Batch 1/4] [D loss: 0.004206] [G loss: 6.413376]\n",
        "[Epoch 590/1900] [Batch 2/4] [D loss: 0.004305] [G loss: 6.485194]\n",
        "[Epoch 590/1900] [Batch 3/4] [D loss: 0.003452] [G loss: 6.651218]\n",
        "[Epoch 600/1900] [Batch 0/4] [D loss: 0.006270] [G loss: 6.290268]\n",
        "[Epoch 600/1900] [Batch 1/4] [D loss: 0.005211] [G loss: 6.373223]\n",
        "[Epoch 600/1900] [Batch 2/4] [D loss: 0.005043] [G loss: 6.337621]\n",
        "[Epoch 600/1900] [Batch 3/4] [D loss: 0.004990] [G loss: 6.419341]\n",
        "[Epoch 610/1900] [Batch 0/4] [D loss: 0.004799] [G loss: 6.348661]\n",
        "[Epoch 610/1900] [Batch 1/4] [D loss: 0.006650] [G loss: 6.279469]\n",
        "[Epoch 610/1900] [Batch 2/4] [D loss: 0.008007] [G loss: 6.083986]\n",
        "[Epoch 610/1900] [Batch 3/4] [D loss: 0.006173] [G loss: 6.013462]\n",
        "[Epoch 620/1900] [Batch 0/4] [D loss: 0.003744] [G loss: 6.394999]\n",
        "[Epoch 620/1900] [Batch 1/4] [D loss: 0.004440] [G loss: 6.550554]\n",
        "[Epoch 620/1900] [Batch 2/4] [D loss: 0.005468] [G loss: 6.450809]\n",
        "[Epoch 620/1900] [Batch 3/4] [D loss: 0.004537] [G loss: 6.317276]\n",
        "[Epoch 630/1900] [Batch 0/4] [D loss: 0.005574] [G loss: 6.299180]\n",
        "[Epoch 630/1900] [Batch 1/4] [D loss: 0.005995] [G loss: 6.291187]\n",
        "[Epoch 630/1900] [Batch 2/4] [D loss: 0.005698] [G loss: 6.420548]\n",
        "[Epoch 630/1900] [Batch 3/4] [D loss: 0.005788] [G loss: 6.376054]\n",
        "[Epoch 640/1900] [Batch 0/4] [D loss: 0.011375] [G loss: 5.916278]\n",
        "[Epoch 640/1900] [Batch 1/4] [D loss: 0.010366] [G loss: 6.004195]\n",
        "[Epoch 640/1900] [Batch 2/4] [D loss: 0.009091] [G loss: 6.373059]\n",
        "[Epoch 640/1900] [Batch 3/4] [D loss: 0.015138] [G loss: 5.473999]\n",
        "[Epoch 650/1900] [Batch 0/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 650/1900] [Batch 1/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 650/1900] [Batch 2/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 650/1900] [Batch 3/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 660/1900] [Batch 0/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 660/1900] [Batch 1/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 660/1900] [Batch 2/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 660/1900] [Batch 3/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 670/1900] [Batch 0/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 670/1900] [Batch 1/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 670/1900] [Batch 2/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 670/1900] [Batch 3/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 680/1900] [Batch 0/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 680/1900] [Batch 1/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 680/1900] [Batch 2/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 680/1900] [Batch 3/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 690/1900] [Batch 0/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 690/1900] [Batch 1/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 690/1900] [Batch 2/4] [D loss: 100.000000] [G loss: 100.000000]\n",
        "[Epoch 690/1900] [Batch 3/4] [D loss: 100.000000] [G loss: 100.000000]\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "ignored",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-22-9be362b109ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Measure discriminator's ability to classify real from generated samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mreal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mfake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#/ 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-9-60ec184f6d86>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mcrit_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcrit_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrit_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "or_Zi_59gdLj"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "b4GjUtL_gdQx"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "yGwSO2_cgdX0"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "jfGha7kOuufd"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "colab": {
       "base_uri": "https://localhost:8080/",
       "height": 132
      },
      "id": "Wm2MmaTl6H3b",
      "outputId": "579f7973-37d7-4a3f-f05a-e93ffa08db8b"
     },
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "ignored",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-41d7b809b646>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    for i, imgs in enumerate(dataloader):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
       ]
      }
     ],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "SnbW0eez981A"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "id": "b0-Cy0pO0dZn"
     },
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSAvDGyRC_C"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "#import cv2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U7xRe-31Gvr",
        "outputId": "8fa57327-7046-439a-a702-fd102b8e3c83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEi2iMTk1im0",
        "outputId": "ac5ac0ac-7f46-4941-f44c-8757612160a7"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/Machine_Learning/GAN/div2k/data/') #/\n",
        "!pwd\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Machine_Learning/GAN/div2k/data\n",
            "\u001b[0m\u001b[01;34mDIV2K_train_HR\u001b[0m/  \u001b[01;34mimages\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiNf1sj2OJel",
        "outputId": "af016a0e-2d8e-4ad0-afdb-38b2d8dd2b7c"
      },
      "source": [
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=1900, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.00001, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=1, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=1000, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=128, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=30, help=\"interval betwen image samples\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=3, img_size=128, latent_dim=1000, lr=1e-05, n_cpu=1, n_epochs=1900, sample_interval=30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6f7RzzZvdfk",
        "outputId": "c0d93035-25f8-4b6b-af53-d6b82f7ae44e"
      },
      "source": [
        "# Normalization parameters for pre-trained PyTorch models\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, files, lr_shape):\n",
        "        lr_height, lr_width = lr_shape\n",
        "        # Transforms for low resolution images and high resolution images\n",
        "        self.lr_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((lr_height, lr_height), Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        self.files = files\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        img_lr = self.lr_transform(img)\n",
        "        return {\"lr\": img_lr}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "# Configure data loader\n",
        "dataset_path = \"DIV2K_train_HR\"\n",
        "lr_shape=(opt.img_size, opt.img_size)\n",
        "train_paths, test_paths = train_test_split(sorted(glob.glob(dataset_path + \"/*.*\")), test_size=0.2)\n",
        "dataloader = DataLoader(ImageDataset(train_paths, lr_shape=lr_shape), batch_size=opt.batch_size, shuffle=True, num_workers=opt.n_cpu, drop_last=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bos7ikTn8FYr"
      },
      "source": [
        "#https://towardsdatascience.com/building-your-own-self-attention-gans-e8c9b9fe8e51\n",
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Construct the conv layers\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)#//2\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        \n",
        "        # Initialize gamma as 0\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax  = nn.Softmax(dim=-1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B * C * W * H)\n",
        "            returns :\n",
        "                out : self attention value + input feature \n",
        "                attention: B * N * N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        \n",
        "        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n",
        "        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product\n",
        "        \n",
        "        attention = self.softmax(energy) # B * N * N\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N\n",
        "        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product\n",
        "        out = out.view(m_batchsize,C,width,height) # B * C * W * H\n",
        "        \n",
        "        # Add attention weights onto input\n",
        "        out = self.gamma*out + x\n",
        "        return out#, attention"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZgKol1B8mQ-"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, out_channels=1):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(opt.latent_dim, 64, 4, 1, 0),#, bias=False\n",
        "            #nn.BatchNorm2d(64*16),\n",
        "            nn.ReLU(True),\n",
        "            # state size. 64*16 x 4 x 4\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1),\n",
        "            #nn.BatchNorm2d(64*8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. 64*8 x 8 x 8\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64*4),\n",
        "            #nn.Dropout(0.2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. 64*4 x 16 x 16\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64*2),\n",
        "            #nn.Dropout(0.2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. 64*2 x 32 x 32\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            #nn.Dropout(0.2),\n",
        "            nn.ReLU(True),\n",
        "            #self attention\n",
        "            #Self_Attn(64),\n",
        "            # state size. 64 x 64 x 64            \n",
        "            nn.ConvTranspose2d(64, opt.channels, 4, 2, 1, ),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 128 x 128\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qx2OCZz-oRV"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, out_channels=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (out_channels) x 128 x 128\n",
        "            nn.Conv2d(opt.channels, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (64) x 64 x 64\n",
        "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64),#*2\n",
        "            #nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. 64*4 x 32 x 32\n",
        "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64),#*4\n",
        "            #nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. 64*8 x 16 x 15\n",
        "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64),#*8\n",
        "            #nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),            \n",
        "            # state size. 64*16 x 8 x 8\n",
        "            nn.Conv2d(64, 64, 4, 2, 1, ),\n",
        "            #nn.BatchNorm2d(64),#*16\n",
        "            #n.Dropout(0.2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            #self attention\n",
        "            #Self_Attn(64),\n",
        "            # state size. 64*16 x 4 x 4\n",
        "            nn.Conv2d(64, 1, 4, 1, 0, ),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Flatten(),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        crit_pred = self.main(input)\n",
        "        return crit_pred.view(len(crit_pred), -1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KRLcWvUB3Un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4cd1b0-bc82-4ab9-a521-703c37dadc8c"
      },
      "source": [
        "# Initialize generator and discriminator\n",
        "from torchsummary import summary\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss() #BCEWithLogitsLoss()#\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "summary(generator, (opt.latent_dim,1,1))\n",
        "summary(discriminator, (opt.channels,128,128))\n",
        "\n",
        "# Configure data loader\n",
        "#dataloader = torch.utils.data.DataLoader(train_images, batch_size=opt.batch_size, num_workers=opt.n_cpu, shuffle=True)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ConvTranspose2d-1             [-1, 64, 4, 4]       1,024,064\n",
            "              ReLU-2             [-1, 64, 4, 4]               0\n",
            "   ConvTranspose2d-3             [-1, 64, 8, 8]          65,600\n",
            "              ReLU-4             [-1, 64, 8, 8]               0\n",
            "   ConvTranspose2d-5           [-1, 64, 16, 16]          65,600\n",
            "              ReLU-6           [-1, 64, 16, 16]               0\n",
            "   ConvTranspose2d-7           [-1, 64, 32, 32]          65,600\n",
            "              ReLU-8           [-1, 64, 32, 32]               0\n",
            "   ConvTranspose2d-9           [-1, 64, 64, 64]          65,600\n",
            "             ReLU-10           [-1, 64, 64, 64]               0\n",
            "  ConvTranspose2d-11          [-1, 3, 128, 128]           3,075\n",
            "             Tanh-12          [-1, 3, 128, 128]               0\n",
            "================================================================\n",
            "Total params: 1,289,539\n",
            "Trainable params: 1,289,539\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 6.08\n",
            "Params size (MB): 4.92\n",
            "Estimated Total Size (MB): 11.00\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           3,136\n",
            "         LeakyReLU-2           [-1, 64, 64, 64]               0\n",
            "            Conv2d-3           [-1, 64, 32, 32]          65,600\n",
            "         LeakyReLU-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5           [-1, 64, 16, 16]          65,600\n",
            "         LeakyReLU-6           [-1, 64, 16, 16]               0\n",
            "            Conv2d-7             [-1, 64, 8, 8]          65,600\n",
            "         LeakyReLU-8             [-1, 64, 8, 8]               0\n",
            "            Conv2d-9             [-1, 64, 4, 4]          65,600\n",
            "        LeakyReLU-10             [-1, 64, 4, 4]               0\n",
            "           Conv2d-11              [-1, 1, 1, 1]           1,025\n",
            "          Flatten-12                    [-1, 1]               0\n",
            "          Sigmoid-13                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 266,561\n",
            "Trainable params: 266,561\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 5.33\n",
            "Params size (MB): 1.02\n",
            "Estimated Total Size (MB): 6.53\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI3ByZ4Bgb2h",
        "outputId": "10e9856b-c4f3-4ff3-f5a6-59bfb5dd077b"
      },
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "\n",
        "    #tqdm_bar = tqdm(dataloader, desc=f'Training Epoch {epoch} ', total=int(len(dataloader)))\n",
        "    for i, imgs in enumerate(dataloader):\n",
        "\n",
        "        # Configure model input\n",
        "        real_imgs = Variable(imgs[\"lr\"].type(Tensor))\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((real_imgs.size(0), 1))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((real_imgs.size(0), 1))), requires_grad=False)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(torch.randn(opt.batch_size,opt.latent_dim, 1, 1, device=DEVICE))#\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) #/ 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        #z_2 = Variable(torch.randn(opt.batch_size,opt.latent_dim, 1, 1, device=DEVICE))#\n",
        "        # Generate a batch of images\n",
        "        #gen_imgs_2 = generator(z_2)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "            )\n",
        "\n",
        "        #batches_done = epoch * len(dataloader) + i\n",
        "        if epoch % opt.sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/%d.png\" % epoch, nrow=5, normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/1900] [Batch 0/5] [D loss: 1.385502] [G loss: 0.691947]\n",
            "[Epoch 0/1900] [Batch 1/5] [D loss: 1.385297] [G loss: 0.691979]\n",
            "[Epoch 0/1900] [Batch 2/5] [D loss: 1.384829] [G loss: 0.692013]\n",
            "[Epoch 0/1900] [Batch 3/5] [D loss: 1.384455] [G loss: 0.692050]\n",
            "[Epoch 0/1900] [Batch 4/5] [D loss: 1.383850] [G loss: 0.692087]\n",
            "[Epoch 10/1900] [Batch 0/5] [D loss: 1.317614] [G loss: 0.692842]\n",
            "[Epoch 10/1900] [Batch 1/5] [D loss: 1.313777] [G loss: 0.692788]\n",
            "[Epoch 10/1900] [Batch 2/5] [D loss: 1.309749] [G loss: 0.692748]\n",
            "[Epoch 10/1900] [Batch 3/5] [D loss: 1.306002] [G loss: 0.692696]\n",
            "[Epoch 10/1900] [Batch 4/5] [D loss: 1.300911] [G loss: 0.692647]\n",
            "[Epoch 20/1900] [Batch 0/5] [D loss: 0.855880] [G loss: 0.707185]\n",
            "[Epoch 20/1900] [Batch 1/5] [D loss: 0.846545] [G loss: 0.708506]\n",
            "[Epoch 20/1900] [Batch 2/5] [D loss: 0.836460] [G loss: 0.709941]\n",
            "[Epoch 20/1900] [Batch 3/5] [D loss: 0.829314] [G loss: 0.711339]\n",
            "[Epoch 20/1900] [Batch 4/5] [D loss: 0.820143] [G loss: 0.712975]\n",
            "[Epoch 30/1900] [Batch 0/5] [D loss: 0.585160] [G loss: 0.857603]\n",
            "[Epoch 30/1900] [Batch 1/5] [D loss: 0.581537] [G loss: 0.862292]\n",
            "[Epoch 30/1900] [Batch 2/5] [D loss: 0.577595] [G loss: 0.868232]\n",
            "[Epoch 30/1900] [Batch 3/5] [D loss: 0.572313] [G loss: 0.874417]\n",
            "[Epoch 30/1900] [Batch 4/5] [D loss: 0.567999] [G loss: 0.880198]\n",
            "[Epoch 40/1900] [Batch 0/5] [D loss: 0.238953] [G loss: 1.714609]\n",
            "[Epoch 40/1900] [Batch 1/5] [D loss: 0.232536] [G loss: 1.738227]\n",
            "[Epoch 40/1900] [Batch 2/5] [D loss: 0.225879] [G loss: 1.762172]\n",
            "[Epoch 40/1900] [Batch 3/5] [D loss: 0.219207] [G loss: 1.785830]\n",
            "[Epoch 40/1900] [Batch 4/5] [D loss: 0.214542] [G loss: 1.807299]\n",
            "[Epoch 50/1900] [Batch 0/5] [D loss: 0.142517] [G loss: 2.084891]\n",
            "[Epoch 50/1900] [Batch 1/5] [D loss: 0.141956] [G loss: 2.087624]\n",
            "[Epoch 50/1900] [Batch 2/5] [D loss: 0.141515] [G loss: 2.093273]\n",
            "[Epoch 50/1900] [Batch 3/5] [D loss: 0.140876] [G loss: 2.094945]\n",
            "[Epoch 50/1900] [Batch 4/5] [D loss: 0.140130] [G loss: 2.099287]\n",
            "[Epoch 60/1900] [Batch 0/5] [D loss: 0.082964] [G loss: 2.611693]\n",
            "[Epoch 60/1900] [Batch 1/5] [D loss: 0.081831] [G loss: 2.624953]\n",
            "[Epoch 60/1900] [Batch 2/5] [D loss: 0.080261] [G loss: 2.644599]\n",
            "[Epoch 60/1900] [Batch 3/5] [D loss: 0.079620] [G loss: 2.654679]\n",
            "[Epoch 60/1900] [Batch 4/5] [D loss: 0.078383] [G loss: 2.669374]\n",
            "[Epoch 70/1900] [Batch 0/5] [D loss: 0.094483] [G loss: 3.451655]\n",
            "[Epoch 70/1900] [Batch 1/5] [D loss: 0.099494] [G loss: 3.443722]\n",
            "[Epoch 70/1900] [Batch 2/5] [D loss: 0.094701] [G loss: 3.449826]\n",
            "[Epoch 70/1900] [Batch 3/5] [D loss: 0.090684] [G loss: 3.483201]\n",
            "[Epoch 70/1900] [Batch 4/5] [D loss: 0.089794] [G loss: 3.546268]\n",
            "[Epoch 80/1900] [Batch 0/5] [D loss: 0.421310] [G loss: 2.407975]\n",
            "[Epoch 80/1900] [Batch 1/5] [D loss: 0.411051] [G loss: 2.380533]\n",
            "[Epoch 80/1900] [Batch 2/5] [D loss: 0.399271] [G loss: 2.469287]\n",
            "[Epoch 80/1900] [Batch 3/5] [D loss: 0.411870] [G loss: 2.531709]\n",
            "[Epoch 80/1900] [Batch 4/5] [D loss: 0.443216] [G loss: 2.290835]\n",
            "[Epoch 90/1900] [Batch 0/5] [D loss: 2.225813] [G loss: 1.012175]\n",
            "[Epoch 90/1900] [Batch 1/5] [D loss: 2.363952] [G loss: 1.126937]\n",
            "[Epoch 90/1900] [Batch 2/5] [D loss: 2.260590] [G loss: 1.177358]\n",
            "[Epoch 90/1900] [Batch 3/5] [D loss: 2.310859] [G loss: 0.999543]\n",
            "[Epoch 90/1900] [Batch 4/5] [D loss: 2.343020] [G loss: 1.118108]\n",
            "[Epoch 100/1900] [Batch 0/5] [D loss: 0.863330] [G loss: 1.545757]\n",
            "[Epoch 100/1900] [Batch 1/5] [D loss: 0.803990] [G loss: 1.846554]\n",
            "[Epoch 100/1900] [Batch 2/5] [D loss: 0.792592] [G loss: 1.669909]\n",
            "[Epoch 100/1900] [Batch 3/5] [D loss: 0.781863] [G loss: 1.642679]\n",
            "[Epoch 100/1900] [Batch 4/5] [D loss: 0.787452] [G loss: 1.765889]\n",
            "[Epoch 110/1900] [Batch 0/5] [D loss: 0.914694] [G loss: 1.515224]\n",
            "[Epoch 110/1900] [Batch 1/5] [D loss: 0.870251] [G loss: 1.501677]\n",
            "[Epoch 110/1900] [Batch 2/5] [D loss: 0.821802] [G loss: 1.464963]\n",
            "[Epoch 110/1900] [Batch 3/5] [D loss: 0.797913] [G loss: 1.514621]\n",
            "[Epoch 110/1900] [Batch 4/5] [D loss: 0.704416] [G loss: 1.663176]\n",
            "[Epoch 120/1900] [Batch 0/5] [D loss: 0.553530] [G loss: 1.958110]\n",
            "[Epoch 120/1900] [Batch 1/5] [D loss: 0.555846] [G loss: 1.969002]\n",
            "[Epoch 120/1900] [Batch 2/5] [D loss: 0.578185] [G loss: 1.902280]\n",
            "[Epoch 120/1900] [Batch 3/5] [D loss: 0.574765] [G loss: 1.952606]\n",
            "[Epoch 120/1900] [Batch 4/5] [D loss: 0.604913] [G loss: 1.884407]\n",
            "[Epoch 130/1900] [Batch 0/5] [D loss: 0.110754] [G loss: 3.206113]\n",
            "[Epoch 130/1900] [Batch 1/5] [D loss: 0.117810] [G loss: 3.196667]\n",
            "[Epoch 130/1900] [Batch 2/5] [D loss: 0.121148] [G loss: 3.161588]\n",
            "[Epoch 130/1900] [Batch 3/5] [D loss: 0.107647] [G loss: 3.199954]\n",
            "[Epoch 130/1900] [Batch 4/5] [D loss: 0.109893] [G loss: 3.232640]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or_Zi_59gdLj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4GjUtL_gdQx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGwSO2_cgdX0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoFUKPy6mCA2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}